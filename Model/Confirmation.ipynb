{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35e0f18",
   "metadata": {},
   "source": [
    "## Before Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81911e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.6.1-cp310-cp310-win_amd64.whl (12.0 MB)\n",
      "     ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/12.0 MB 6.9 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.8/12.0 MB 9.8 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.3/12.0 MB 10.4 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.9/12.0 MB 11.3 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 2.3/12.0 MB 10.6 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 3.2/12.0 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.0/12.0 MB 12.7 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 4.8/12.0 MB 13.3 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.4/12.0 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.9/12.0 MB 13.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 6.8/12.0 MB 13.5 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.3/12.0 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 7.8/12.0 MB 13.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 8.6/12.0 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.2/12.0 MB 13.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 9.6/12.0 MB 13.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.1/12.0 MB 13.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 10.7/12.0 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 11.3/12.0 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  11.8/12.0 MB 13.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.0/12.0 MB 13.1 MB/s eta 0:00:00\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-win_amd64.whl (94 kB)\n",
      "     ---------------------------------------- 0.0/94.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 94.7/94.7 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 0.0/48.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.9/48.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (2.28.2)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp310-cp310-win_amd64.whl (481 kB)\n",
      "     ---------------------------------------- 0.0/481.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 481.6/481.6 kB 31.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (66.0.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.12-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ------------------------- -------------- 1.0/1.5 MB 20.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 15.8 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "     ---------------------------------------- 0.0/370.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 370.9/370.9 kB 11.3 MB/s eta 0:00:00\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (23.0)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting pydantic-core==2.4.0\n",
      "  Downloading pydantic_core-2.4.0-cp310-none-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ---------------- ----------------------- 0.7/1.7 MB 22.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.4/1.7 MB 17.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.4/1.7 MB 17.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 10.9 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp310-cp310-win_amd64.whl (7.4 MB)\n",
      "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.4/7.4 MB 13.9 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.8/7.4 MB 9.8 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 1.3/7.4 MB 10.5 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 1.9/7.4 MB 11.0 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 2.4/7.4 MB 10.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 2.7/7.4 MB 10.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.3/7.4 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 3.9/7.4 MB 11.3 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.4/7.4 MB 11.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 5.0/7.4 MB 11.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 5.6/7.4 MB 11.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 6.1/7.4 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.6/7.4 MB 11.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 7.2/7.4 MB 11.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.4/7.4 MB 11.3 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.1-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, catalogue, blis, annotated-types, typer, srsly, pydantic-core, preshed, pydantic, pathy, confection, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "Successfully installed annotated-types-0.5.0 blis-0.7.10 catalogue-2.0.9 confection-0.1.1 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-2.1.1 pydantic-core-2.4.0 smart-open-6.3.0 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 thinc-8.1.12 typer-0.9.0 typing-extensions-4.7.1 wasabi-1.1.2\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Before running!\n",
    "# !pip install tensorflow==2.10.1\n",
    "# !pip install keras==2.10.0\n",
    "# !pip install scapy==3.6.1\n",
    "\n",
    "# You need to do this once\n",
    "# spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35a1b6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd0cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462826ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Responses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm looking for some information about Deakin ...</td>\n",
       "      <td>Courses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have a question about the admission requirem...</td>\n",
       "      <td>Courses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I want to know more about Deakin scholarships....</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I would like to know more about the campus clu...</td>\n",
       "      <td>Activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am interested in studying psychology at Deak...</td>\n",
       "      <td>Courses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>What facilities are available on Campus?</td>\n",
       "      <td>Campus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>How many Campuses Does Deakin University have?</td>\n",
       "      <td>Campus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>On which campus I can study IT?</td>\n",
       "      <td>Campus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>Where Can I stay near Deakin?</td>\n",
       "      <td>Accomodation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>Where Can I stay near Deakin?</td>\n",
       "      <td>Accomodation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>738 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Responses         Class\n",
       "0    I'm looking for some information about Deakin ...       Courses\n",
       "1    I have a question about the admission requirem...       Courses\n",
       "2    I want to know more about Deakin scholarships....       General\n",
       "3    I would like to know more about the campus clu...    Activities\n",
       "4    I am interested in studying psychology at Deak...       Courses\n",
       "..                                                 ...           ...\n",
       "733           What facilities are available on Campus?        Campus\n",
       "734     How many Campuses Does Deakin University have?        Campus\n",
       "735                    On which campus I can study IT?        Campus\n",
       "736                      Where Can I stay near Deakin?  Accomodation\n",
       "737                      Where Can I stay near Deakin?  Accomodation\n",
       "\n",
       "[738 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from the Excel file into a DataFrame\n",
    "data = pd.read_excel(\"response.xlsx\") \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b78e44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "Courses         448\n",
       "Accomodation    162\n",
       "Activities       58\n",
       "General          43\n",
       "Campus           27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7de2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences and classes from the DataFrame\n",
    "sentences = data[\"Responses\"].tolist()\n",
    "classes = data[\"Class\"].tolist()\n",
    "\n",
    "# Lowercase sentences\n",
    "sentences = [sentence.lower() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a7332",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789dfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences and extract word vectors\n",
    "X = np.array([nlp(sentence).vector for sentence in sentences])\n",
    "\n",
    "# Convert class labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(classes)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fc1af",
   "metadata": {},
   "source": [
    "### Saving the label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b3cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Load the encoder\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a30520",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18193d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "590/590 [==============================] - 3s 5ms/step - loss: 0.8261 - accuracy: 0.7593\n",
      "Epoch 2/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.3930 - accuracy: 0.8814\n",
      "Epoch 3/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.2994 - accuracy: 0.8932\n",
      "Epoch 4/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.2397 - accuracy: 0.9102\n",
      "Epoch 5/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.2005 - accuracy: 0.9339\n",
      "Epoch 6/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1261 - accuracy: 0.9593\n",
      "Epoch 7/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1937 - accuracy: 0.9390\n",
      "Epoch 8/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1678 - accuracy: 0.9542\n",
      "Epoch 9/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.0994 - accuracy: 0.9763\n",
      "Epoch 10/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1115 - accuracy: 0.9593\n",
      "Epoch 11/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.0637 - accuracy: 0.9729\n",
      "Epoch 12/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1158 - accuracy: 0.9678\n",
      "Epoch 13/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.0824 - accuracy: 0.9678\n",
      "Epoch 14/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1048 - accuracy: 0.9712\n",
      "Epoch 15/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.0737 - accuracy: 0.9729\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.9392\n",
      "Test Accuracy: 0.9391891956329346\n"
     ]
    }
   ],
   "source": [
    "# Build a neural network model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(512, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca721e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a `.keras` zip archive.\n",
    "model.save('Classification_93.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be97e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('Classification_93.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b935d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "Accomodation\n",
      "[0.9999138]\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp('Where can I stay at deakin?'.lower()).vector\n",
    "predicted_labels = model.predict(np.array([sentence]))\n",
    "predicted_label = label_encoder.inverse_transform(predicted_labels.argmax(axis=1))[0]\n",
    "print(predicted_label)\n",
    "\n",
    "predicted_prob = predicted_labels[0][predicted_labels.argmax(axis=1)]\n",
    "print(predicted_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b1d79",
   "metadata": {},
   "source": [
    "## Flask Function Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd7238b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abv_thresh': True, 'label': 'Courses'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.post(\n",
    "    'http://10.104.23.130:8891/classifyResponse',\n",
    "    json={\n",
    "        'sentence': 'I want to know about courses related to AI and Data Science',\n",
    "        'threshold': 0.95\n",
    "    }\n",
    ")\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc144707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Courses'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()['label']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
