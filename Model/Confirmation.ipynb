{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81911e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.6.1-cp310-cp310-win_amd64.whl (12.0 MB)\n",
      "     ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/12.0 MB 6.9 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.8/12.0 MB 9.8 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.3/12.0 MB 10.4 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.9/12.0 MB 11.3 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 2.3/12.0 MB 10.6 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 3.2/12.0 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.0/12.0 MB 12.7 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 4.8/12.0 MB 13.3 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.4/12.0 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.9/12.0 MB 13.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 6.8/12.0 MB 13.5 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.3/12.0 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 7.8/12.0 MB 13.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 8.6/12.0 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.2/12.0 MB 13.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 9.6/12.0 MB 13.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.1/12.0 MB 13.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 10.7/12.0 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 11.3/12.0 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  11.8/12.0 MB 13.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.0/12.0 MB 13.1 MB/s eta 0:00:00\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-win_amd64.whl (94 kB)\n",
      "     ---------------------------------------- 0.0/94.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 94.7/94.7 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 0.0/48.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.9/48.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (2.28.2)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp310-cp310-win_amd64.whl (481 kB)\n",
      "     ---------------------------------------- 0.0/481.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 481.6/481.6 kB 31.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (66.0.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.12-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ------------------------- -------------- 1.0/1.5 MB 20.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 15.8 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "     ---------------------------------------- 0.0/370.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 370.9/370.9 kB 11.3 MB/s eta 0:00:00\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from spacy) (23.0)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting pydantic-core==2.4.0\n",
      "  Downloading pydantic_core-2.4.0-cp310-none-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ---------------- ----------------------- 0.7/1.7 MB 22.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.4/1.7 MB 17.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.4/1.7 MB 17.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 10.9 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp310-cp310-win_amd64.whl (7.4 MB)\n",
      "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.4/7.4 MB 13.9 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.8/7.4 MB 9.8 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 1.3/7.4 MB 10.5 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 1.9/7.4 MB 11.0 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 2.4/7.4 MB 10.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 2.7/7.4 MB 10.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.3/7.4 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 3.9/7.4 MB 11.3 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.4/7.4 MB 11.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 5.0/7.4 MB 11.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 5.6/7.4 MB 11.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 6.1/7.4 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.6/7.4 MB 11.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 7.2/7.4 MB 11.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.4/7.4 MB 11.3 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.1-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sudak\\anaconda3\\envs\\tf\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, catalogue, blis, annotated-types, typer, srsly, pydantic-core, preshed, pydantic, pathy, confection, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "Successfully installed annotated-types-0.5.0 blis-0.7.10 catalogue-2.0.9 confection-0.1.1 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-2.1.1 pydantic-core-2.4.0 smart-open-6.3.0 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 thinc-8.1.12 typer-0.9.0 typing-extensions-4.7.1 wasabi-1.1.2\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "# Download and install the spaCy model\n",
    "import spacy\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "# spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd0cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462826ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Responses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm looking for some information about Deakin ...</td>\n",
       "      <td>Courses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have a question about the admission requirem...</td>\n",
       "      <td>Courses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I want to know more about Deakin scholarships....</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I would like to know more about the campus clu...</td>\n",
       "      <td>Activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am interested in studying psychology at Deak...</td>\n",
       "      <td>Courses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Responses       Class\n",
       "0  I'm looking for some information about Deakin ...     Courses\n",
       "1  I have a question about the admission requirem...     Courses\n",
       "2  I want to know more about Deakin scholarships....     General\n",
       "3  I would like to know more about the campus clu...  Activities\n",
       "4  I am interested in studying psychology at Deak...     Courses"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from the Excel file into a DataFrame\n",
    "data = pd.read_excel(\"response.xlsx\") \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b78e44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "Courses         448\n",
       "Accomodation    162\n",
       "Activities       58\n",
       "General          43\n",
       "Campus           27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7de2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences and classes from the DataFrame\n",
    "sentences = data[\"Responses\"].tolist()\n",
    "classes = data[\"Class\"].tolist()\n",
    "\n",
    "# Lowercase sentences\n",
    "sentences = [sentence.lower() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93d30d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "590/590 [==============================] - 2s 2ms/step - loss: 0.6789 - accuracy: 0.8186\n",
      "Epoch 2/10\n",
      "590/590 [==============================] - 1s 2ms/step - loss: 0.2791 - accuracy: 0.8983\n",
      "Epoch 3/10\n",
      "590/590 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.9407\n",
      "Epoch 4/10\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1431 - accuracy: 0.9525\n",
      "Epoch 5/10\n",
      "590/590 [==============================] - 3s 5ms/step - loss: 0.1493 - accuracy: 0.9407\n",
      "Epoch 6/10\n",
      "590/590 [==============================] - 3s 6ms/step - loss: 0.1201 - accuracy: 0.9610\n",
      "Epoch 7/10\n",
      "590/590 [==============================] - 3s 5ms/step - loss: 0.0848 - accuracy: 0.9695\n",
      "Epoch 8/10\n",
      "590/590 [==============================] - 4s 7ms/step - loss: 0.1086 - accuracy: 0.9695\n",
      "Epoch 9/10\n",
      "590/590 [==============================] - 4s 6ms/step - loss: 0.0841 - accuracy: 0.9797\n",
      "Epoch 10/10\n",
      "590/590 [==============================] - 3s 5ms/step - loss: 0.0729 - accuracy: 0.9780\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5341 - accuracy: 0.8716\n",
      "Test Accuracy: 0.8716216087341309\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "Sentence: Where can I stay at Deakin?, Predicted Class: Accomodation\n",
      "Sentence: What course can I take in AI?, Predicted Class: Courses\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences and extract word vectors\n",
    "X = np.array([nlp(sentence).vector for sentence in sentences])\n",
    "\n",
    "# Convert class labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(classes)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a neural network model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(300, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Test the model with new sentences\n",
    "new_sentences = [\"Where can I stay at Deakin?\", \"What course can I take in AI?\"]\n",
    "new_X = np.array([nlp(sentence).vector for sentence in new_sentences])\n",
    "predicted_labels = model.predict(new_X)\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels.argmax(axis=1))\n",
    "\n",
    "for sentence, predicted_class in zip(new_sentences, predicted_classes):\n",
    "    print(f\"Sentence: {sentence}, Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "789dfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences and extract word vectors\n",
    "X = np.array([nlp(sentence).vector for sentence in sentences])\n",
    "\n",
    "# Convert class labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(classes)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18193d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "590/590 [==============================] - 6s 4ms/step - loss: 0.8709 - accuracy: 0.7542\n",
      "Epoch 2/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.4021 - accuracy: 0.8644\n",
      "Epoch 3/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.3079 - accuracy: 0.9034\n",
      "Epoch 4/15\n",
      "590/590 [==============================] - 2s 3ms/step - loss: 0.2303 - accuracy: 0.9186\n",
      "Epoch 5/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.2222 - accuracy: 0.9305\n",
      "Epoch 6/15\n",
      "590/590 [==============================] - 2s 3ms/step - loss: 0.2004 - accuracy: 0.9305\n",
      "Epoch 7/15\n",
      "590/590 [==============================] - 2s 3ms/step - loss: 0.1825 - accuracy: 0.9492\n",
      "Epoch 8/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1098 - accuracy: 0.9593\n",
      "Epoch 9/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1409 - accuracy: 0.9593\n",
      "Epoch 10/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1673 - accuracy: 0.9373\n",
      "Epoch 11/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.0699 - accuracy: 0.9763\n",
      "Epoch 12/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.1193 - accuracy: 0.9627\n",
      "Epoch 13/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.0697 - accuracy: 0.9763\n",
      "Epoch 14/15\n",
      "590/590 [==============================] - 2s 3ms/step - loss: 0.2022 - accuracy: 0.9559\n",
      "Epoch 15/15\n",
      "590/590 [==============================] - 2s 4ms/step - loss: 0.0865 - accuracy: 0.9678\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9527\n",
      "Test Accuracy: 0.9527027010917664\n"
     ]
    }
   ],
   "source": [
    "# Build a neural network model\n",
    "model_2 = tf.keras.Sequential()\n",
    "model_2.add(tf.keras.layers.Dense(512, input_dim=X_train.shape[1], activation='relu'))\n",
    "model_2.add(tf.keras.layers.Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model_2.add(tf.keras.layers.Dropout(0.25))\n",
    "model_2.add(tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "model_2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_2.fit(X_train, y_train, epochs=15, batch_size=1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model_2.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca721e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a `.keras` zip archive.\n",
    "model_2.save('Classification_93.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be97e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('Classification_93.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757aa014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step\n",
      "Sentence: Where can I stay at Deakin?, Predicted Class: General\n",
      "Sentence: What course can I take in AI?, Predicted Class: Courses\n"
     ]
    }
   ],
   "source": [
    "# Test the model with new sentences\n",
    "new_sentences = [\"Where can I stay at Deakin?\", \"What course can I take in AI?\"]\n",
    "new_X = np.array([nlp(sentence).vector for sentence in new_sentences])\n",
    "predicted_labels = model_2.predict(new_X)\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels.argmax(axis=1))\n",
    "\n",
    "for sentence, predicted_class in zip(new_sentences, predicted_classes):\n",
    "    print(f\"Sentence: {sentence}, Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78c11bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Campus', 'Courses'], dtype='<U12')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebcf3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with new sentences\n",
    "\n",
    "def say_2(strg):\n",
    "    new_sentences = [strg]\n",
    "    new_X = np.array([nlp(sentence).vector for sentence in new_sentences])\n",
    "    predicted_labels = model_2.predict(new_X)\n",
    "    predicted_classes = label_encoder.inverse_transform(predicted_labels)\n",
    "    \n",
    "    for i, probability in enumerate(predicted_labels[0]):\n",
    "        print(f\"Class {classes[i]}: Probability {probability*100:.4f}\")\n",
    "#     for sentence, predicted_class in zip(new_sentences, predicted_classes):\n",
    "#         print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83bd6602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3993125e-12, 1.8401272e-11, 3.3813549e-12, 6.4059485e-08,\n",
       "        9.9999988e-01]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9781f536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model_2.predict(np.array([nlp(\"Give me accomodation\").vector]))\n",
    "predicted_labels\n",
    "x = np.argmax(predicted_labels[0])\n",
    "predicted_classes = label_encoder.inverse_transform([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0087d82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['General'], dtype='<U12')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3baff67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (1, 5) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msay_2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGive me Accomodation services in Deakin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[49], line 7\u001b[0m, in \u001b[0;36msay_2\u001b[1;34m(strg)\u001b[0m\n\u001b[0;32m      5\u001b[0m new_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([nlp(sentence)\u001b[38;5;241m.\u001b[39mvector \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m new_sentences])\n\u001b[0;32m      6\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m model_2\u001b[38;5;241m.\u001b[39mpredict(new_X)\n\u001b[1;32m----> 7\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, probability \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predicted_labels[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Probability \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobability\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:155\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"Transform labels back to original encoding.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    Original encoding.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 155\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# inverse transform of empty array is empty array\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1202\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1193\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1194\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1198\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1199\u001b[0m         )\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1204\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (1, 5) instead."
     ]
    }
   ],
   "source": [
    "say_2(\"Give me Accomodation services in Deakin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f9cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eeecc63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "Class Courses: Probability 98.6150\n",
      "Class Courses: Probability 0.0000\n",
      "Class General: Probability 1.3848\n",
      "Class Activities: Probability 0.0000\n",
      "Class Courses: Probability 0.0001\n"
     ]
    }
   ],
   "source": [
    "say_2(\"Where can I stay at Deakin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16f36816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Courses\n"
     ]
    }
   ],
   "source": [
    "say_2(\"Give me soume course related to AI at Deakin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d426f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "Campus\n"
     ]
    }
   ],
   "source": [
    "say_2(\"How many Campuses does Deakin have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f56c875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Class Courses: Probability 0.0000\n",
      "Class Courses: Probability 0.0000\n",
      "Class General: Probability 0.0000\n",
      "Class Activities: Probability 0.0001\n",
      "Class Courses: Probability 99.9999\n"
     ]
    }
   ],
   "source": [
    "say_2(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88e6b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "Class Courses: Probability 0.0237\n",
      "Class Courses: Probability 0.0672\n",
      "Class General: Probability 0.0035\n",
      "Class Activities: Probability 45.2903\n",
      "Class Courses: Probability 54.6154\n"
     ]
    }
   ],
   "source": [
    "say_2(\"Stop how are you going today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7238b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
